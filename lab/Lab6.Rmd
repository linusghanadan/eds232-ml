---
title: "Lab6"
author: "Linus Ghanadan"
date: "2023-03-01"
output: html_document
---

## Case Study: Eel Distribution Modeling

This week's lab follows a project modeling the eel species Anguilla australis described by Elith et al. (2008). There are two data sets for this lab.  You'll use one for training and evaluating your model, and you'll use your model to make predictions predictions on the other.  Then you'll compare your model's performance to the model used by Elith et al.


```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, message = FALSE, warning = FALSE)
```


```{r}
# load packages
library(here)
library(tidyverse)
library(tidymodels)
```


## Data

Grab the training data sets (eel.model.data.csv, eel.eval.data.csv) from github here:
https://github.com/MaRo406/eds-232-machine-learning/blob/main/data 

```{r}
# read in data
eel_data <- read.csv(here::here("lab", "data", "eel.model.data.csv")) %>%
  janitor::clean_names() %>% 
  mutate(angaus = as.factor(angaus)) %>% 
  select(-site)
```


### Split and Resample

Split the model data (eel.model.data.csv) into a training and test set, stratified by outcome score (Angaus). Use 10-fold CV to resample the training set.

```{r}
# set seed for reproducibility
set.seed(123)

# initial split of data (default 70/30)
eel_split <- initial_split(eel_data, strata = angaus, prop = 0.7)
eel_test <- testing(eel_split)
eel_train <- training(eel_split)
```

```{r}
# create 10 folds of the training dataset for CV
cv_folds <- eel_train %>% vfold_cv(v = 10)
```



### Preprocess

Create a recipe to prepare your data for the XGBoost model

```{r}
# specify recipe for model preprocessing
eel_recipe <- recipe(angaus ~ ., data = eel_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  prep() # prep recipe

# bake training data using recipe
baked_train <- bake(eel_recipe, eel_train)
```


## Tuning XGBoost

### Tune Learning Rate

Following the XGBoost tuning strategy outlined in lecture, first we conduct tuning on just the learning rate parameter:

1.  Create a model specification using {xgboost} for the estimation

-   Only specify one parameter to tune()

```{r}
# specify model for tuning learning rate
xgboost_spec <- boost_tree(mode = "classification",
                           engine = "xgboost",
                           learn_rate = tune())
                            
```


2.  Set up a grid to tune your model by using a range of learning rate parameter values: expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

-   Use appropriate metrics argument(s) - Computational efficiency becomes a factor as models get more complex and data get larger. Record the time it takes to run. Do this for each tuning phase you run.You could use {tictoc} or Sys.time().

```{r}
# create tuning grid for learning rate
lr_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))
```


3.  Show the performance of the best models and the estimates for the learning rate parameter values associated with each.

```{r}
# create workflow for tuning learning rate
xgboost_workflow <- workflow() %>%
  add_model(xgboost_spec) %>%
  add_recipe(eel_recipe)
```

```{r}
# tune the model for optimal learning rate
xgboost_tune <- tune_grid(
  xgboost_workflow,
  resamples = cv_folds,
  grid = lr_grid
)
```


```{r}
# store optimized learning rate
best_lr <- select_best(xgboost_tune, "accuracy")
best_lr$learn_rate
```


### Tune Tree Parameters

1.  Create a new specification where you set the learning rate (which you already optimized) and tune the tree parameters.

```{r}
# specify model with optimized learning rate to tune tree-based parameters
xgboost_spec_tree <- boost_tree(
  trees = 3000,
  tree_depth = tune(),
  min_n = tune(),
  learn_rate = best_lr$learn_rate,
  loss_reduction = tune(),
  mode = "classification",
  engine = "xgboost"
)
```

2.  Set up a tuning grid. This time use grid_latin_hypercube() to get a representative sampling of the parameter space

```{r}
# create tuning grid for tree-based parameters
tree_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  size = 10)


```


3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}
# create workflow for tuning tree-based parameters
xgboost_workflow_tree <- workflow() %>%
  add_model(xgboost_spec_tree) %>%
  add_recipe(eel_recipe)

# tune tree-based parameters
xgboost_tree_tune <- tune_grid(
  xgboost_workflow_tree,
  resamples = cv_folds,
  grid = tree_grid
)
```

```{r}
# collect metrics for best models
best_models_metrics <- xgboost_tree_tune %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>% # assuming accuracy is your metric of interest
  arrange(desc(mean)) # arrange by the highest mean accuracy

# store parameters of best model
best_tree_params <- select_best(xgboost_tree_tune, "accuracy")

# show metrics of best models
print(best_models_metrics)

```



### Tune Stochastic Parameters

1.  Create a new specification where you set the learning rate and tree parameters (which you already optimized) and tune the stochastic parameters.

```{r}
# specify model with optimized learning rate and tree parameters to tune based on stochastic parameters
xgboost_spec_stochastic <- boost_tree(
  trees = 3000,
  tree_depth = best_tree_params$tree_depth,
  min_n = best_tree_params$min_n,
  learn_rate = best_lr$learn_rate,
  loss_reduction = best_tree_params$loss_reduction,
  mtry = tune(),
  sample_size = tune(),
  mode = "classification",
  engine = "xgboost"
)
```



2.  Set up a tuning grid. Use grid_latin_hypercube() again.

```{r}
# finalize mtry range based on the number of predictors
mtry_finalized <- finalize(mtry(), eel_train)

# create tuning grid for stochastic parameters
stochastic_grid <- grid_latin_hypercube(
  mtry_finalized,
  sample_size = sample_prop(),
  size = 10
)

```

3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}
# create workflow for tuning stochastic parameters
xgboost_workflow_stochastic <- workflow() %>%
  add_model(xgboost_spec_stochastic) %>%
  add_recipe(eel_recipe)

# tune stochastic parameters
xgboost_stochastic_tune <- tune_grid(
  xgboost_workflow_stochastic,
  resamples = cv_folds,
  grid = stochastic_grid
)
```

```{r}
# collect metrics for best models
best_stochastic_metrics <- xgboost_stochastic_tune %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  arrange(desc(mean)) # arrange by highest mean accuracy

# store parameters of best model
best_stochastic_params <- select_best(xgboost_stochastic_tune, "accuracy")

# show metrics of best models
print(best_stochastic_metrics)

```



## Finalize workflow and make final prediction

```{r}
# specify final model with optimized parameters
final_xgboost_model <- finalize_model(
  xgboost_spec_stochastic,
  best_stochastic_params
)

# fit final model to training data
final_fit <- fit(
  final_xgboost_model,
  formula = angaus ~ .,
  data = eel_train
)
```

```{r}

# predict testing data
test_predict <- predict(final_fit, eel_test) %>%
  bind_cols(eel_test) # bind predictions column to testing data

# create confusion matrix
conf_mat(data = test_predict, truth = angaus, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")
```


1.  How well did your model perform? What types of errors did it make?

```{r}
# get accuracy, sensitivity, and specificity
accuracy(test_predict, truth = angaus, estimate = .pred_class)
sensitivity(test_predict, truth = angaus, estimate = .pred_class)
specificity(test_predict, truth = angaus, estimate = .pred_class)
```


[When applied to the testing data, the accuracy of the model was 78%. However, the model made a lot of type I (false-negative) errors (i.e., when there were eels at the site, the model only correctly predicted that this was the case about 43% of the time)]{style="color:navy;"}

## Fit your model the evaluation data and compare performance

1.  Now used your final model to predict on the other dataset (eval.data.csv)

```{r}
# load evaluation data
eel_eval_data <- read.csv(here::here("lab", "data", "eel.eval.data.csv")) %>%
  janitor::clean_names() %>%
  mutate(angaus = as.factor(angaus_obs)) %>% 
  select(-angaus_obs)

# preprocess evaluation data using same recipe
baked_eval <- bake(eel_recipe, eel_eval_data)

```

```{r}
# predict evaluation data
eval_predict <- predict(final_fit, baked_eval) %>% 
    bind_cols(eel_test) # bind predictions column to testing data

# create confusion matrix
conf_mat(data = eval_predict, truth = angaus, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")

```



2.  How does your model perform on this data?

```{r}
# get accuracy, sensitivity, and specificity
accuracy(eval_predict, truth = angaus, estimate = .pred_class)
sensitivity(eval_predict, truth = angaus, estimate = .pred_class)
specificity(eval_predict, truth = angaus, estimate = .pred_class)
```


3.  How do your results compare to those of Elith et al.?

-   Use {vip} to compare variable importance
-   What do your variable importance results tell you about the distribution of this eel species?
