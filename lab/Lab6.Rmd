---
title: "Lab6"
author: "Linus Ghanadan"
date: "2023-03-01"
output: html_document
---

## Case Study: Eel Distribution Modeling

This week's lab follows a project modeling the eel species Anguilla australis described by Elith et al. (2008). There are two data sets for this lab.  You'll use one for training and evaluating your model, and you'll use your model to make predictions predictions on the other.  Then you'll compare your model's performance to the model used by Elith et al.

```{r}
library(here)
library(tidyverse)
library(tidymodels)
```


## Data

Grab the training data sets (eel.model.data.csv, eel.eval.data.csv) from github here:
https://github.com/MaRo406/eds-232-machine-learning/blob/main/data 

```{r}
eel_data <- read.csv(here::here("lab", "data", "eel.model.data.csv")) %>%
  janitor::clean_names() %>% 
  mutate(angaus = as.factor(angaus))
```


### Split and Resample

Split the model data (eel.model.data.csv) into a training and test set, stratified by outcome score (Angaus). Use 10-fold CV to resample the training set.

```{r}
# set seed for reproducibility
set.seed(123)

# initial split of data (default 70/30)
eel_split <- initial_split(eel_data, strata = angaus, prop = 0.7)
eel_test <- testing(eel_split)
eel_train <- training(eel_split)
```

```{r}
# create 10 folds of the training dataset for CV
cv_folds <- eel_train %>% vfold_cv(v = 10)
```



### Preprocess

Create a recipe to prepare your data for the XGBoost model

```{r}
# specify recipe for model preprocessing
eel_recipe <- recipe(angaus ~ ., data = eel_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  prep() # prep recipe

# bake training data using recipe
baked_train <- bake(eel_recipe, eel_train)
```


## Tuning XGBoost

### Tune Learning Rate

Following the XGBoost tuning strategy outlined in lecture, first we conduct tuning on just the learning rate parameter:

1.  Create a model specification using {xgboost} for the estimation

-   Only specify one parameter to tune()

```{r}
# specify model
xgboost_spec <- boost_tree(mode = "classification",
                           engine = "xgboost",
                           learn_rate = tune())
                            
```


2.  Set up a grid to tune your model by using a range of learning rate parameter values: expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

-   Use appropriate metrics argument(s) - Computational efficiency becomes a factor as models get more complex and data get larger. Record the time it takes to run. Do this for each tuning phase you run.You could use {tictoc} or Sys.time().

```{r}
lr_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))
```


3.  Show the performance of the best models and the estimates for the learning rate parameter values associated with each.

```{r}
# create workflow
xgboost_workflow <- workflow() %>%
  add_model(xgboost_spec) %>%
  add_recipe(eel_recipe)
```

```{r}
# tune the model
xgboost_tune <- tune_grid(
  xgboost_workflow,
  resamples = cv_folds,
  grid = lr_grid
)
```


```{r}
# determine optimal learning rate and store
best_lr <- select_best(xgboost_tune, "accuracy")
best_lr$learn_rate
```


### Tune Tree Parameters

1.  Create a new specification where you set the learning rate (which you already optimized) and tune the tree parameters.

```{r}
# Create a new model specification with the optimized learning rate
# and parameters to tune for tree complexity
xgboost_spec_tree <- boost_tree(
  trees = 3000, # Set a high number of trees for now; we'll use early stopping to find the optimal number
  tree_depth = tune(), # or use max_depth = tune() depending on your version of tidymodels/xgboost
  min_n = tune(),
  learn_rate = best_lr$learn_rate, # use the optimized learning rate
  loss_reduction = tune(),
  mode = "classification",
  engine = "xgboost"
)
```

2.  Set up a tuning grid. This time use grid_latin_hypercube() to get a representative sampling of the parameter space

```{r}
# create a tuning grid
tree_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  size = 10)


```


3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}
# Create workflow and tune the model with the new specification
xgboost_workflow_tree <- workflow() %>%
  add_model(xgboost_spec_tree) %>%
  add_recipe(eel_recipe)

# Tune the tree-based parameters
xgboost_tree_tune <- tune_grid(
  xgboost_workflow_tree,
  resamples = cv_folds,
  grid = tree_grid
)
```

```{r}
# Collect and show the metrics for the best models
best_models_metrics <- xgboost_tree_tune %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>% # assuming accuracy is your metric of interest
  arrange(desc(mean)) # arrange by the highest mean accuracy

# Show the best models' metrics
print(best_models_metrics)

# To see the parameters for the best model (or models if you want to inspect more)
# best_models_parameters <- xgboost_tree_tune %>%
#   collect_parameters() %>%
#   arrange(desc(mean_accuracy)) %>%
#   select(tree_depth, min_n, everything()) # adjust this line based on the parameters you tuned
# 
# # Show the best models' parameters
# print(best_models_parameters)
```

```{r}
# Additionally, to get the exact parameters of the best model, use select_best()
best_tree_params <- select_best(xgboost_tree_tune, "accuracy")

# Print the best parameters
print(best_tree_params)
```


### Tune Stochastic Parameters

1.  Create a new specification where you set the learning rate and tree parameters (which you already optimized) and tune the stochastic parameters.

2.  Set up a tuning grid. Use grid_latin_hypercube() again.

3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

## Finalize workflow and make final prediction

1.  How well did your model perform? What types of errors did it make?

## Fit your model the evaluation data and compare performance

1.  Now used your final model to predict on the other dataset (eval.data.csv)

2.  How does your model perform on this data?

3.  How do your results compare to those of Elith et al.?

-   Use {vip} to compare variable importance
-   What do your variable importance results tell you about the distribution of this eel species?
