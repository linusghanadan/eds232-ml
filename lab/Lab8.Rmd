---
title: "Lab 8"
author: "Mateo Robbins"
date: "2024-03-06"
output: html_document
---

## Forest Cover Classification with SVM

In this week's lab we are exploring the use of Support Vector Machines for multi-class classification. Specifically, you will be using cartographic variables to predict forest cover type (7 types).

Natural resource managers responsible for developing ecosystem management strategies require basic descriptive information including inventory data like forest cover type for forested lands to support their decision-making processes. However, managers generally do not have this type of data for in-holdings or neighboring lands that are outside their immediate jurisdiction. One method of obtaining this information is through the use of predictive models.

You task is build both an SVM and a random forest model and compare their performance on accuracy and computation time.

1.  The data is available here: <https://ucsb.box.com/s/ai5ost029enlguqyyn04bnlfaqmp8kn4>

Explore the data.

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, message = FALSE, warning = FALSE)
```

```{r}
# load libraries
library(tidymodels)
library(tidyverse)
```

```{r}
# set seed
set.seed(123)
```

```{r}
# import data
covtype_df <- readr::read_csv(here::here("lab/data/covtype_sample.csv")) %>% 
  janitor::clean_names() %>% 
  mutate(cover_type = as.factor(cover_type)) %>% # convert cover_type column to un-ordered factor
  select(-soil_type_15) # remove b/c only one factor level
```

```{r}
# check distribution of outcome variable
summary(covtype_df$wilderness_area_rawah)
```


-   What kinds of features are we working with?

-   Does anything stand out that will affect you modeling choices? Hint: Pay special attention to the distribution of the outcome variable across the classes.

[The outcome variable is very skewed right. For this reason, its important that we normalize the outcome variable by using a classification method]{style="color:navy;"}

2.  Create the recipe and carry out any necessary preprocessing. Can you use the same recipe for both models?

```{r}
# initial split of data (default 70/30)
covtype_split <- initial_split(covtype_df, strata = cover_type, prop = 0.7)
covtype_test <- testing(covtype_split)
covtype_train <- training(covtype_split)
```


```{r}
# specify recipe for model preprocessing
covtype_recipe <- recipe(cover_type ~ ., data = covtype_train) %>%
  step_dummy(all_nominal_predictors(), -all_outcomes()) %>%
  step_normalize(all_numeric_predictors()) %>%
  prep() # prep recipe

```


3.  Create the folds for cross-validation.

```{r}
# create 10 folds of the training dataset for CV
cv_folds <- covtype_train %>% vfold_cv(v = 10)
```


4.  Tune the models. Choose appropriate parameters and grids. If the computational costs of tuning given your strategy are prohibitive, how might you work around this?

## Random forest model

```{r}
# specify RF model
rf_spec <- rand_forest(trees = 500, mtry = tune(), min_n = tune()) %>%
  set_engine("ranger") %>%
  set_mode("classification")
```

```{r}
# create a tuning grid
rf_grid <- grid_latin_hypercube(
  mtry(range = c(2, 4)), 
  min_n(c(1, 10)),
  size = 20
)
```

```{r}
# create workflow
wf_rf <- workflow() %>%
  add_recipe(covtype_recipe) %>%
  add_model(rf_spec)
```

```{r}
# tune the model
rf_tune_res <- tune_grid(
  wf_rf,
  resamples = cv_folds,
  grid = rf_grid
)
```


```{r}
# finalize the model
best_rf_params <- select_best(rf_tune_res, "accuracy")
final_rf <- finalize_model(rf_spec, best_rf_params)
```



```{r}
# finalize the model with optimal parameters
final_rf <- finalize_model(rf_spec, best_rf_params)

# create a new workflow with finalized model
final_rf_workflow <- workflow() %>%
  add_recipe(covtype_recipe) %>%
  add_model(final_rf)

# fit final workflow to the training data
final_rf_fit <- final_rf_workflow %>%
  fit(data = covtype_train)
```

5.  Conduct final predictions for both models and compare their prediction performances and computation costs from part 4.


```{r}
# predict testing data
test_predict_rf <- predict(final_rf_fit, covtype_test) %>%
  bind_cols(covtype_test) %>%
  mutate(cover_type = as.factor(cover_type))

 # get prediction probabilities for test
# test_predict_rf <- predict(final_rf_fit, tracks_test, type = "prob") %>%
#   bind_cols(test_predict_rf) %>%
#   mutate(name = as.factor(name))

# make confusion matrix for predictions made on testing data
test_predict_rf %>%
  conf_mat(truth = cover_type, estimate = .pred_class) %>%
  autoplot(type = "heatmap") +
  ggtitle("RF") +
  theme_bw() +
  theme(legend.position = "none")
```

```{r}
# store error metrics of testing data predictions
accuracy_rf <- accuracy(test_predict_rf, truth = cover_type, estimate = .pred_class)
#roc_auc_rf <- roc_auc(test_predict_rf, truth = name, .pred_linus)
```


-   Which type of model do you think is better for this task?
-   Why do you speculate this is the case?
